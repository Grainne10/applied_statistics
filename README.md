# applied_statistics
Coursework for Applied Statistics - Data Analytics
**by Grainne Boyle**

**README Content:**

1. [Overview](README.md/#overview)
2. [Contents](README.md/#contents)
3. [Installation](README.md/#installation)
3. [Problems](README.md/#problems)
4. [Research](README.md/#research)

## Overview

I am a student at the [Atlantic Technological University](https://www.atu.ie/), Galway, studying the Higher Diploma in Science in Data Analytics on a part-time basis over 2 years. This repository contains my coursework for the Applied Statistics module, completed as part of the Winter 25/26 assessment.  
The purpose of the project is to demonstrate my understanding of statistical concepts by applying statistical methods, simulation techniques, and analytical workflows in Python. All work is presented in a Jupyter notebook, which includes clear explanations, commentary, and visualisations to support the statistical results.  
The assessment covers four problems:
- Extending Fishers' Lady Tasting Tea experiment
- Sampling variability under the normal distribution
- Analysing type II error rates in t-tests
- Comparing ANOVA with multiple pairwise tests.  

The notebook is designed to present clear reasoning, well explained methods, and results that reflect both the statistical theory and what I’ve learned in the course.  

## Contents  
`problems.ipynb`: A Jupyter notebook with an explanation of how I completed the tasks, including command usage and data analysis.  
`README.md`: Provides an overview of the project, explains its purpose, and describes how to run the code in this repository.  
`img/`: A directory containing images used in the project.  
`requirements`: A text file listing the dependencies required to run the project, including libraries such as pandas and jupyter.   
`.gitignore`: Specifies files and directories that should be ignored by Git.     

## Installation  

To run this project, Python and Jupyter Notebook must be installed on your system.
All required Python packages are listed in the `requirements.txt` file.
First, clone or download this repository to your local machine.
Next, open a terminal in the project directory and install the required dependencies using `pip install -r requirements.txt`.
Once the dependencies are installed, start `jupyter notebook` by running jupyter notebook.
Finally, open the project notebook and run the cells from top to bottom to reproduce the analysis and results.

## Problems

### Problem 1 - Extending  the Lady Tasting Tea Experiment
This problem extends Fisher's *Lady Tasting Tea* experiment.  
In this experiment, the null hypothesis is that the lady cannot truly distinguish between tea-first and milk-first cups, and that any correct identification is due to random guessing. This hypothesis is assumed to be true unless there is strong evidence against it. When a result is statistically significant, it means the evidence is unlikely to be explained by chance alone, so we reject the null hypothesis.

In the original experiment, there are 8 cups in total, with 4 tea-first and 4 milk-first. The probability of correctly identifying all 4 tea-first cups by chance is:

$$
P = \frac{1}{\binom{8}{4}} = \frac{1}{70} \approx 0.0143
$$

In the extended experiment, there are 12 cups in total, with 8 tea-first, 4 milk-first, the number of possible combinations is:

$$
\binom{12}{8} = \frac{12!}{8!(12-8)!} = 495
$$

Therefore, the probability of correctly identifying all 8 tea-first cups by chance is:

$$
P = \frac{1}{495} \approx 0.0020.
$$

This shows that it is much more difficult to correctly identify all cups in the extended experiment.

To verify the theoretical probability, the experiment was simulated in Python using NumPy. In each simulation, the cups were randomly shuffled many times and the lady's choices were generated by random guessing. The simulation result matched the theoretical probability very closely, confirming the accuracy of our calculation. The computer experiment confirmed that our mathematical answer was correct.

Under the null hypothesis, a perfect score should be very unlikely. The participant is only guessing and has no real ability to tell the difference between tea-first and milk-first cups. In the original experiment, the probability of success by chance is approximately 1.43%, whereas in the extended 12-cup experiment it is approximately 0.2%. This indicates that the  extended experiment provides stronger evidence that a perfect score is not due to random guessing alone.

### Problem 2 - Normal Distribution

In this problem, I explored how the standard deviation behaves when sampling from a standard normal distribution. I generated 100,000 samples, each samples containing 10 values. For every sample, I computed two versions of the standard deviation, one using `ddof=0` and one using `ddof=1`(delta degrees of freedom). The results were plotted on a histogram with transparency to allow visual comparison of their sampling distributions.

The results show that when the sample size is small(n=10), the `ddof=0` estimator tends to underestimate the true standard deviation of the population. This occurs because dividing by n does not account for the loss of one degree of freedom when the sample mean is calculated. In contrast , the `ddof=1` estimator divides by n-1, producing slightly larger values on average. As a result, its histogram is shifted slightly to the right and its peak is closer to the true value of 1. This highlights that sample standard deviation `ddof=1` is unbiased, while the `ddof=0` is biased downward when the sample sizes are small.

To examine the effect of sample size, the simulation was repeated using samples of size 100. With the larger sample size, the two histograms overlapped more closely. Both estimators produced values tightly clustered around 1, and the difference between `ddof=0` and `ddof=1` was very small. This confirms that as sample size increases, the impact of the degrees of freedom adjustment decreases and both estimators converge to the true population standard deviation.

This problem demonstrates key ideas related to sampling distributions, estimator bias, and the role of degrees of freedom in statistics. It shows how repeated sampling behaves in practice and reinforces theoretical results through simulation.

### Problem 3 -  T-tests

In this problem, the focus is on understanding **Type II error rates** and how they are affected by the size of the true mean differences between populations.  Simulation was used to investigate how often an independent samples t-test fails to detect a real difference between two population means.   
Two independent samples were repeatedly drawn, each group had a sample size of `n = 100`. One sample was drawn from a standard normal distribution,`N(0, 1)`, while the second sample was drawn from a normal distribution with the same variance but with its mean shifted by a value `d`, that is `N(d, 1)`. The value of `d` was varied from `0` to `1` in increments of `0.1` to represent increasing differences between the population means.     
For each value of `d`, the experiment was repeated 1000 times. In each simulation, a two-sample independent t-test was performed for each simulation at a significance level of `α = 0.05`. The null hypothesis stated that there is no difference between the population means of the two groups,`H₀: μ_A = μ_B`, while the alternative hypothesis
stated that the population means are different, `H₁: μ_A ≠ μ_B`.  
When `d = 0`, both samples come from identical distributions and the null hypothesis is true. As `d` increases, a real difference between the populations means differ and the null hypothesis becomes false. A **Type II error** occurs when the test fails to reject the null hypothesis even though this real difference is present.  
The results show that when `d` is small the Type II error rate is high. In these cases, the two populations are very similar and random sampling variation makes it difficult for the t-test to detect the difference. As `d` increases, the populations become more separated and the test becomes more likely to reject the null hypothesis. As a result, the Type II error decreases rapidly and approaches zero for larger values of `d`.  
This problem demonstrates the relationship between Type II errors, effect size and statistical power. While the t-test performs well for moderate to large differences, it can miss small but real effects. Understanding this tradeoff is essential in real-world applications such as scientific research, psychology, and medical testing, where failing to detect a true effect can have serious consequences.

### Problem 4 - ANOVA

This problem investigates the difference between using a one-way `ANOVA` and multiple two-sample t-tests when comparing more than two group means. Three independent samples were generated, each with a sample size of `n = 30`, from normal distributions with standard deviation of `σ = 1` and means of `0`,`0.5` and `1`, respectively.  
Using these samples:  
- A one-way `ANOVA` was performed to test the null hypothesis that all three population means are equal. 
- Three indendepent two sample t-tests were conducted to compare each pair of samples(A vs B, A vs C and B vs C)
- A `Tukey HSD` post-hoc test was applied following the `ANOVA` to inderify which specific group means differed.
- A boxplot was created to visually compare the distributions of the three samples.

The results show how one-way `ANOVA` provides a single global test for detecting differences among multiple group means while controlling the overall Type I error rate. Although multiple t-tests can indentify pairwise differences, running several t-tests increases the probabilities of false positives. In this example, the `ANOVA` indicated a statistically significant difference among the three group means. The `Tukey HSD` test and the pairwise t-tests showed that the difference was driven by the comparison between the samples with means `0.5` and `1`(sample_b and sample_c). The other pairwise comparisons were not statistically significant.  
This illustrates why `ANOVA` is preferred over multiple t-tests. Running multiple t-tests without a global test increases the family-wise error rate, which is the probability of making at least one Type 1 error. For three tests performed at a significance level of `α = 0.05`, the family-wise error rate is
`FWER = 1 − (1 − 0.05)^3 ≈ 0.143` meaning there is a 14.3% chance of at least one false positive.  
Overall, this problem demonstrates that `ANOVA` is a more appropriate and reliable approach for comparing multiple group means , as it controls the risk of false positives while still allowing meaningful dfferences to be identified through post-hoc testing.  


## Research
1.  [Lady tasting tea](https://en.wikipedia.org/wiki/Lady_tasting_tea) - This explains where this test originated.  
2.  ​[LaTeX Math in Jupyter Documentation](https://jupyterbook.org/en/stable/content/math.html) - I used this in my jupyter notebook to improve the appearance of formulas etc.  
3.  [NullHypothesis](https://www.youtube.com/watch?v=DAkJhY2zQ3c) - This video explains very clearly what the null hypothesis, statistical significance and p-value are.  
4.  [Random seed](https://www.w3schools.com/python/ref_random_seed.asp) - Used random seeds in my Lady Tasting Tea function.  
5.  [Random seeds and reproducibility](https://medium.com/data-science/random-seeds-and-reproducibility-933da79446e3) - Article about random seeds and reproducibility.   
6.  [DocStrings]( https://www.geeksforgeeks.org/python/python-docstrings/) –  A `docstring` is a string literal placed at the beginning of a function, class, or module.  
7.  [P-values](https://www.youtube.com/watch?v=vemZtEM63GY) - This  video explains the concept of p-values and statistical significance.  
8.  [KDE](https://www.geeksforgeeks.org/data-science/seaborn-kdeplot-a-comprehensive-guide/) - Understanding the KDE plot.  
9.  [Type II Error](https://corporatefinanceinstitute.com/resources/data-science/type-ii-error/#:~:text=%22False%20negative%22%20error,Increase%20the%20sample%20size) - Explains a Type II error.  
10. [ANOVA](https://www.youtube.com/watch?v=0NwA9xxxtHw) - this video explains that Anova is a test that can be used when there are more than two samples.  
11. [Anova v t-tests](https://www.geeksforgeeks.org/data-science/difference-between-t-test-and-anova/) - explains the difference between these approaches.   
12. [Type 1 error](https://corporatefinanceinstitute.com/resources/data-science/type-i-error/) occurs when a test incorrectly rejects a true null hypothesis.  
13. [f-statistic](https://www.machinelearningplus.com/statistics/f-statistic-formula-explained/) - This article explains how the F-statisic compares how much the group means differ from each other (between-groups variability) to how much variation exists within each group.  
14. [Boxplots](https://www.youtube.com/watch?v=nV8jR8M8C74) - This video provides a clear explanation of boxplots, including how to interpret the minimum, first quartile, median, third quartile, maximum values and the outliers or whiskers.  
15. [CHATGPT](https://chatgpt.com/) - I used these AI tools for some guidance and re-wording on my project. All thoughts and ideas are my own, I spent alot of time researching and reading up on all the topics in the project.  


----------------------------------------
END

![teaimage](img/tea.jpg)

