# applied_statistics
Coursework for Applied Statistics - Data Analytics
**by Grainne Boyle**

![teaimage](img/tea.jpg)

**README Content:**

1. [Overview](README.md/#overview)
2. [Contents](README.md/#repository-contents)
3. [Problems](README.md/#tasks)

## Overview

I am a student at the [Atlantic Technological University](https://www.atu.ie/), Galway, studying the Higher Diploma in Science in Data Analytics on a part-time basis over 2 years. This repository contains my coursework for the Applied Statistics module, completed as part of the Winter 25/26 assessment.  
The purpose of the project is to demonstrate my understanding of statistical concepts by applying statistical methods, simulation techniques, and analytical workflows in Python. All work is presented in a Jupyter notebook, which includes clear explanations, commentary, and visualisations to support the statistical results.  
The assessment covers four problems:
- Extending Fishers' Lady Tasting Tea experiment
- Sampling variability under the normal distribution
- Analysing type II error rates in t-tests
- Comparing ANOVA with multiple pairwise tests.  
The notebook is designed to present clear reasoning, well explained methods, and results that reflect both the statistical theory and what I’ve learned in the course.  

## Contents
`problems.ipynb`: A a Jupyter notebook with an explanation of how I completed the tasks, including command usage and data analysis.  
`README.md`: Provides an overview of the project, explains its purpose, and describes how to run the code in this repository.
`img/`: An image folder containing images I used in the project.
`requirements/`: A text file listing the dependencies required to run the project, including libraries such as pandas and jupyter.  


## Problems

### Problem 1 - Extending  the Lady Tasting Tea Experiment
This problem extends Fisher's *Lady Tasting Tea* experiment.  
In this experiment, the null hypothesis is that the lady cannot truly distinguish between tea-first and milk-first cups, and that any correct identification is due to random guessing. This hypothesis is assumed to be true unless there is strong evidence against it. When a result is statistically significant, it means the evidence is unlikely to be explained by chance alone, so we reject the null hypothesis.

In the original experiment, there are 8 cups in total, with 4 tea-first and 4 milk-first. The probability of correctly identifying all 4 tea-first cups by chance is:

$$
P = \frac{1}{\binom{8}{4}} = \frac{1}{70} \approx 0.0143
$$

In the extended experiment, there are 12 cups in total, with 8 tea-first, 4 milk-first, the number of possible combinations is:

$$
\binom{12}{8} = \frac{12!}{8!(12-8)!} = 495
$$

Therefore, the probability of correctly identifying all 8 tea-first cups by chance is:

$$
P = \frac{1}{495} \approx 0.0020.
$$

This shows that is much more difficult to correctly identify all cups in the extended experiment.

To verify the theoretical probability, the experiment was simulated in Python using NumPy. In each simulation, the cups were randomly shuffled many times and the lady's choicees were generated by random guessing. The simulation result matched the theoretical probability very closely, confirming the accuracy of our calculation. The computer experiment confirmed that our mathematical answer was correct.

Under the null hypothesis, a perfect score should be very unlikely . The participant is only guessing and has no real ability to tell the difference between tea-first and milk-first cups. In the original experiment, the probability of success by chance is approximately 1.43%, whereas in the extended 12-cup experiment it is approximately 0.2%. This indicates that the  extended experiment provides stronger evidence that a perfect score is not due to random guessing alone.

### Problem 2 - Normal Distribution

In this problem, I explored how the standard deviation behaves when sampling from a standard normal distribution. I generated 100,000 samples, each samples containing 10 values. For every sample, I computed two versions of the standard deviation, one using `ddof=0` and one using `ddof=1`(delta degrees of freedom). The results were plotted on a histogram with transparency to allow visual comparison of their sampling distributions.

The results show that when the sample size is small(n=10), the `ddof=0` estimator tends to underestimate the true standard deviation of the population. This occurs because dividing by n does not account for the loss of one degree of freedom when the sample mean is calculated. In contrast , the `ddof=1` estimator by n-1, producing slightly larger values on average. As a result, its histogram is shifted slightly to the right and its peak is closer to the true value of 1. This highlights that sample standard deviation `ddof=1` is unbiased, while the `ddof=0` is biased downward when the sample sizes are small.

To examine the effect of sample size, the simulation was repeated using samples of size 100. With the larger sample size, the two histograms overlapped more closely. Both estimators produced values tightly clustered around 1, and the difference between `ddof=0` and `ddof=1` were very small. This confirms that as sample size increases, the impact of the degrees of freedom adjustment decreases and both estimators converge to the true population standard deviation.

This problem demonstrates key ideas related to sampling distributions, estimator bias, and the role of degrees of freedom in statistics. It shows how repeated sampling behaves in practice and reinforces theoretical results through simulation.

### Problem 3 -  T-tests

In this problem, I explored how Type II error rates behave when performing an independent samples t-test under different true mean differences between two populations. 
The purpose of this problem is to investigate the relationship between effect size, statistical power, and Type II errors. It demonstrates how sensitive the t-test is to differences between two population means.

Two independent samples were repeatedly drawn, each group had a sample size of n =100 and the experiment was repeated 1000 times:
Group A was sampled from a standard normal distribution:
N(0,1)N(0,1)N(0,1)
Group B was sampled from a normal distribution with the same variance but a shifted mean:
N(d,1)N(d, 1)N(d,1)
A two-sample independent t-test was performed for each simulation at a significance level of α=0.05\alpha = 0.05α=0.05. explain alpha
Null Hypothesis (H0H_0H0​):
There is no difference between the population means of Group A and Group B.
μA=μB\mu_A = \mu_BμA​=μB​
Hypothesis
The population means of Group A and Group B are different.
μA≠μB\mu_A \neq \mu_BμA​=μB​
When d=0d = 0d=0, both groups come from identical distributions and the null hypothesis is true. As ddd increases, the null hypothesis becomes false, and a real difference between the populations exists.

Type II Error Explanation:
A Type II error occurs when a hypothesis test fails to reject the null hypothesis even though it is false. This is also known as a false negative.   In this simulation, a Type II error happens when the t-test does not detect a real difference between the two population means when d>0d > 0d>0.
A real-world example of a Type II error would be a medical test that incorrectly indicates a patient does not have a disease when they actually do. The consequence is that the condition goes untreated. Similarly, in statistics, failing to detect a real effect can lead to incorrect conclusions and poor decision-making.

Results and Interpretation:
The table of results shows the estimated false negative rate for different values of the true mean difference ddd:
When d=0d = 0d=0, the null hypothesis is true, and the test correctly fails to reject it most of the time (about 94%).
For small values of ddd (e.g., 0.1 or 0.2), the false negative rate is still high. This means the t-test struggles to distinguish between the two populations when the difference is subtle.
As ddd increases, the false negative rate decreases rapidly.
For larger values of ddd (around 0.7 and above), the false negative rate approaches zero, indicating that the t-test almost always detects the real difference.
This behavior shows that the power of the t-test increases as the true effect size increases. Power is defined as 1−Type II error rate1 - \text{Type II error rate}1−Type II error rate, so lower false negative rates correspond to higher power.
The plot visualizes the relationship between true mean difference (effect size ddd) and the Type II error rate. This  shows a steep decline in false negatives as ddd increases. When the populations are very similar, the test frequently misses the effect. As the populations become more separated, the test becomes more reliable.
This visualization reinforces the idea that statistical tests are not equally sensitive to all effect sizes. Detecting small effects requires larger sample sizes or more powerful testing strategies.

Conclusion

This problem demonstrates how Type II errors depend strongly on effect size and illustrates the practical limitations of hypothesis testing. While the t-test performs well for moderate to large differences, it can miss small but real effects. Understanding this tradeoff is essential in real-world applications such as scientific research, psychology, and medical testing, where failing to detect a true effect can have serious consequences.

### Problem 4 - ANOVA

In this problem, I investigate the difference between using a one-way ANOVA and multiple two-sample t-tests when comparing more that two group means.
Three independependent samples, each of size 30, were generated from normal distributions with standard deviation 1 and means od 0,0.5, and 1, respectively. Using these samples:  
- A one-way ANOVA was performed to test the null hypothesis that all three population means are equal. 
- Three indendepent two sample t-tests were conducted to compare each pair of samples(A vs B, A vs C and B vs C)
- A Tukey's HSD post-hoc test was applied following the ANOVA to inderify which specific group means differed.
- A boxplot was created to visually compart the distributions of the three samples.

This problem demonstrates how one-way Anova provides a single global test for detecting differnences among multiple group means while controlling the overall Type I error rate. Although multiple t-tests can indentify pairwise differences, running several t-tests increases the probabilities of false positives.
In this example, the ANOVA indicated a statistically significant difference among the three group means. Using Tukey's HSE and pairwise t-tests showed that the significant difference was driven by the comparison between the samples with means 0.5 and 1(sample_b and sample_c). The other pairwise comparisons were not statistically significant.
ANOVA is preferred over multiple t-tests. Running multiple t-tests without a global test increases the risk of false positives. This is called family-wise error rate.As the result is consistent across all methods, this significant result is unlikely to be a false positive.
ANOVA is preferred over multiple t-tests. Running multiple t-tests without a global test increases the risk of false positives. This is called family-wise error rate.
FWER = 1 − (1 − 0.05)^3 ≈ 0.143 (14.3% chance of at least one false positive)
This shows that running three t-tests at α = 0.05 increases the probability of obtaining at least one false positive to about 14%, even if all null hypotheses are true.

 
----------------------------------------
END

![statistics](img/statistics.jpg)

